Using cpu device
Logging to tensorboard_log/PPO_1
/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x119fae480> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x11ec2c7a0>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
-----------------------------
| time/              |      |
|    fps             | 5629 |
|    iterations      | 1    |
|    time_elapsed    | 1    |
|    total_timesteps | 8192 |
-----------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 3752        |
|    iterations           | 2           |
|    time_elapsed         | 4           |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.012211037 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.256      |
|    explained_variance   | 0.0111      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.666       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00203    |
|    std                  | 0.253       |
|    value_loss           | 34.1        |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 3314        |
|    iterations           | 3           |
|    time_elapsed         | 7           |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.011025099 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.265      |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.744       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00205    |
|    std                  | 0.253       |
|    value_loss           | 6.21        |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 3091        |
|    iterations           | 4           |
|    time_elapsed         | 10          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.013178598 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.242      |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.453       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00546    |
|    std                  | 0.251       |
|    value_loss           | 4.23        |
-----------------------------------------
Traceback (most recent call last):
  File "/Users/jaiselsingh/Desktop/research/PPO.py", line 99, in <module>
    main()
  File "/Users/jaiselsingh/Desktop/research/PPO.py", line 85, in main
    model.learn(total_timesteps=100_000, callback=callbacks) #callback=WandbCallback(verbose=2))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
           ^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 224, in collect_rollouts
    if not callback.on_step():
           ^^^^^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py", line 114, in on_step
    return self._on_step()
           ^^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py", line 223, in _on_step
    continue_training = callback.on_step() and continue_training
                        ^^^^^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py", line 114, in on_step
    return self._on_step()
           ^^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py", line 464, in _on_step
    episode_rewards, episode_lengths = evaluate_policy(
                                       ^^^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py", line 97, in evaluate_policy
    new_observations, rewards, dones, infos = env.step(actions)
                                              ^^^^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 222, in step
    return self.step_wait()
           ^^^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 59, in step_wait
    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]
                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/stable_baselines3/common/monitor.py", line 94, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py", line 393, in step
    return super().step(action)
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/gymnasium/core.py", line 327, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py", line 285, in step
    return self.env.step(action)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/jaiselsingh/Desktop/research/ur5_env.py", line 85, in step
    self.do_simulation(action, self.frame_skip)
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/gymnasium/envs/mujoco/mujoco_env.py", line 199, in do_simulation
    self._step_mujoco_simulation(ctrl, n_frames)
  File "/Users/jaiselsingh/Desktop/research/.venv/lib/python3.12/site-packages/gymnasium/envs/mujoco/mujoco_env.py", line 147, in _step_mujoco_simulation
    mujoco.mj_step(self.model, self.data, nstep=n_frames)
KeyboardInterrupt
